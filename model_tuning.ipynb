{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import langchain.llms as llms\n",
    "from langchain.chains import create_sql_query_chain\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "import openpyxl\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory: C:\\Users\\nithi\\OneDrive - University of Illinois Chicago\\Documents\\Capstone project\\LLM project\n"
     ]
    }
   ],
   "source": [
    "working_dir = \"C:/Users/nithi/OneDrive - University of Illinois Chicago/Documents/Capstone project/LLM project\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "print(f\"The current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite database connection\n",
    "db = SQLDatabase.from_uri(\"sqlite:///dataset.db\")\n",
    "\n",
    "# GPT connection\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base prompt\n",
    "base_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"You are a SQLite expert and Machine Learning Engineer. \n",
    "    Given an input question, create a syntactically correct SQLite query to run, to answer the input question.\n",
    "    \\n Only use the following tables: {table_info}.Question: {input}.Generate up to {top_k} SQL queries to answer the question.\"\"\",\n",
    ")\n",
    "\n",
    "# Define the metric prompt\n",
    "metric_prompt = \"\"\"**Metric Selection Instructions:**\n",
    "\n",
    "* For questions about detailed performance metrics (accuracy, MAE, recall, precision), you MUST JOIN the 'models' table with the 'model_metrics_view' on the MODEL_ID column.\n",
    "\n",
    "* Check the model type first:\n",
    "\n",
    "   * For Regression models, use MAE from model_metrics_view.\n",
    "\n",
    "   * For Classification models, use accuracy, precision, and recall from model_metrics_view.\n",
    "\n",
    "    **Example:**  \n",
    "\n",
    "    Question: Which models have recall greater than 0.9?\n",
    "\n",
    "    SELECT Model_Name \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE Model_type IN ('Multi-Class', 'Classification') and accuracy > 0.9;\n",
    "  \n",
    "\n",
    "    Question: What is the accuracy of Model 10?\n",
    "\n",
    "     SELECT accuracy \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE lower(Model_Name) = 'model 10' and Model_type IN ('Multi-Class', 'Classification');\n",
    "    Answer: Model 10 is a Regression model type so it accuracy is not the right metric to calculate performace of the model\n",
    "\"\"\"\n",
    "\n",
    "# Define the data validation prompt\n",
    "data_validation_prompt = \"\"\"**Data Validation Instructions:**\n",
    "* Before executing the query, ensure that the referenced data likely exists in the database.\n",
    "* If potential issues are detected, indicate the problem and suggest alternatives instead of executing the query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prompt to answer the questions\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question in conversational tone.\n",
    "\n",
    "      If the SQL result is empty, provide a helpful message indicating that no matching data was found.  \n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "\n",
    "Answer: \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input', 'table_info', 'top_k'] template='You are a SQLite expert and Machine Learning Engineer. \\n    Given an input question, create a syntactically correct SQLite query to run, to answer the input question.\\n    \\n Only use the following tables: {table_info}.Question: {input}.Generate up to {top_k} SQL queries to answer the question.'\n"
     ]
    }
   ],
   "source": [
    "# Combine the prompts using FewShotPromptTemplate\n",
    "def generate_prompt(query):\n",
    "    prompt = base_prompt\n",
    "\n",
    "    # Analyze user query for keywords\n",
    "    keywords = [\"accuracy\", \"recall\", \"precision\", \"MAE\", 'model', 'best', 'worst']\n",
    "    detected_metrics = [metric for metric in keywords if metric in query.lower()]\n",
    "\n",
    "    # Add conditional prompts based on detected metrics\n",
    "    if detected_metrics:\n",
    "        prompt = FewShotPromptTemplate(\n",
    "            examples=[\n",
    "                FewShotPromptTemplate.create_example(\n",
    "                    base_prompt.template,\n",
    "                    metric_prompt,\n",
    "                ),\n",
    "                FewShotPromptTemplate.create_example(\n",
    "                    base_prompt.template,\n",
    "                    data_validation_prompt,\n",
    "                ),\n",
    "            ],\n",
    "            prefix=prompt.template,\n",
    "            suffix=\"\\nSQL Query:\",\n",
    "            example_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt = generate_prompt(\"query\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chain 1: Generate SQL Query\n",
    "generate_sql_chain = create_sql_query_chain(prompt=generate_prompt(\"dummy_query\"), llm=llm, db=db)\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "# Initialize Chain 2: Execute SQL Query and generate structured answer\n",
    "execute_sql_chain = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "def calculate_token_size(text):\n",
    "    # Split text into tokens and count the total number of tokens\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "def execute_combined_chain(question):\n",
    "    # Step 1: Generate SQL Query\n",
    "    sql_query = generate_sql_chain.invoke({\"question\": question, \"top_k\": 1})\n",
    "\n",
    "    # Step 2: Execute the SQL Query to get the result\n",
    "    sql_result = execute_query(sql_query)  # Ensure this returns the result of executing the SQL query\n",
    "\n",
    "    # Step 3: Pass the necessary inputs to the final chain and format the output to include both SQL query and result\n",
    "    final_response = execute_sql_chain.invoke({\"question\": question, \"query\": sql_query, \"result\": sql_result})\n",
    "\n",
    "    # Calculate token size of input and output\n",
    "    input_token_size = calculate_token_size(question)\n",
    "    output_token_size = calculate_token_size(final_response)\n",
    "    total_token_size = input_token_size + output_token_size\n",
    "    print(total_token_size)\n",
    "    \n",
    "    # Prompt user if total token size exceeds 10K tokens\n",
    "    if total_token_size > 10000:\n",
    "        return \"Your query is too complex. Please try asking in a simpler way or split it into multiple questions.\"\n",
    "\n",
    "    # Format the final answer to include both the SQL query and its result\n",
    "    final_answer = f\"SQL Query: {sql_query}\\n Answer: {final_response}\"\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n",
      "SQL Query: SELECT Model_Name, Model_Version, MAX(Performance_Metrics) as Best_Performance\n",
      "FROM models\n",
      "GROUP BY Model_Name;\n",
      " Answer: The best version for each model based on their performance metrics are as follows: Model 1's best version is 1, Model 10's best version is 2, Model 11's best version is 1, Model 12's best version is 2, Model 13's best version is 3, Model 14's best version is 1, Model 15's best version is 1, Model 16's best version is 1, Model 17's best version is 2, Model 18's best version is 2, Model 19's best version is 1, Model 2's best version is 1, Model 20's best version is 2, Model 21's best version is 2, Model 22's best version is 1, Model 23's best version is 3, Model 24's best version is 3, Model 25's best version is 1, Model 26's best version is 2, Model 27's best version is 2, Model 28's best version is 3, Model 29's best version is 2, Model 3's best version is 3, Model 30's best version is 1, Model 31's best version is 3, Model 32's best version is 3, Model 33's best version is 2, Model 34's best version is 3, Model 35's best version is 3, Model 36's best version is 3, Model 37's best version is 1, Model 38's best version is 2, Model 39's best version is 1, Model 4's best version is 3, Model 40's best version is 1, Model 41's best version is 2, Model 42's best version is 3, Model 43's best version is 1, Model 44's best version is 1, Model 45's best version is 2, Model 46's best version is 1, Model 47's best version is 1, Model 48's best version is 3, Model 49's best version is 1, Model 5's best version is 2, Model 50's best version is 1, Model 6's best version is 1, Model 7's best version is 2, Model 8's best version is 3, and Model 9's best version is 1.\n"
     ]
    }
   ],
   "source": [
    "# Example invocation\n",
    "question = \"Which model version is better in each model\"\n",
    "final_answer = execute_combined_chain(question)\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
