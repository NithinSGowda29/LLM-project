{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "import langchain.llms as llms\n",
    "from langchain.chains import create_sql_query_chain\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "import openpyxl\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory: C:\\Users\\nithi\\OneDrive - University of Illinois Chicago\\Documents\\Capstone project\\LLM project\n"
     ]
    }
   ],
   "source": [
    "working_dir = \"C:/Users/nithi/OneDrive - University of Illinois Chicago/Documents/Capstone project/LLM project\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "print(f\"The current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite database connection\n",
    "db = SQLDatabase.from_uri(\"sqlite:///dataset.db\")\n",
    "\n",
    "# GPT connection\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base prompt\n",
    "base_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"You are a SQLite expert and Machine Learning Engineer. \n",
    "    Given an input question, create a syntactically correct SQLite query to run, to answer the input question.\n",
    "    \\n Only use the following tables: {table_info}.Question: {input}.Generate up to {top_k} SQL queries to answer the question.\"\"\",\n",
    ")\n",
    "\n",
    "# Define the metric prompt\n",
    "metric_prompt = \"\"\"**Metric Selection Instructions:**\n",
    "\n",
    "* For questions about detailed performance metrics (accuracy, MAE, recall, precision), you MUST JOIN the 'models' table with the 'model_metrics_view' on the MODEL_ID column.\n",
    "\n",
    "* Check the model type first:\n",
    "\n",
    "   * For Regression models, use MAE from model_metrics_view.\n",
    "\n",
    "   * For Classification models, use accuracy, precision, and recall from model_metrics_view.\n",
    "\n",
    "    **Example:**  \n",
    "\n",
    "    Question: Which models have recall greater than 0.9?\n",
    "\n",
    "    SELECT Model_Name \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE Model_type IN ('Multi-Class', 'Classification') and accuracy > 0.9;\n",
    "  \n",
    "\n",
    "    Question: What is the accuracy of Model 10?\n",
    "\n",
    "     SELECT accuracy \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE lower(Model_Name) = 'model 10' and Model_type IN ('Multi-Class', 'Classification');\n",
    "    Answer: Model 10 is a Regression model type so it accuracy is not the right metric to calculate performace of the model\n",
    "\"\"\"\n",
    "\n",
    "# Define the data validation prompt\n",
    "data_validation_prompt = \"\"\"**Data Validation Instructions:**\n",
    "* Before executing the query, ensure that the referenced data likely exists in the database.\n",
    "* If potential issues are detected, indicate the problem and suggest alternatives instead of executing the query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Prompt to answer the questions\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question in conversational tone.\n",
    "\n",
    "      If the SQL result is empty, provide a helpful message indicating that no matching data was found.  \n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "\n",
    "Answer: \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'FewShotPromptTemplate' has no attribute 'create_example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m\n\u001b[0;32m     11\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     12\u001b[0m             examples\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     13\u001b[0m                 FewShotPromptTemplate\u001b[38;5;241m.\u001b[39mcreate_example(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m             example_prompt\u001b[38;5;241m=\u001b[39mbase_prompt,\n\u001b[0;32m     25\u001b[0m         )\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n\u001b[1;32m---> 29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the accuracy of model A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m, in \u001b[0;36mgenerate_prompt\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Add conditional prompts based on detected metrics\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_metrics:\n\u001b[0;32m     11\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m FewShotPromptTemplate(\n\u001b[0;32m     12\u001b[0m         examples\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 13\u001b[0m             \u001b[43mFewShotPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_example\u001b[49m(\n\u001b[0;32m     14\u001b[0m                 base_prompt\u001b[38;5;241m.\u001b[39mtemplate,\n\u001b[0;32m     15\u001b[0m                 metric_prompt,\n\u001b[0;32m     16\u001b[0m             ),\n\u001b[0;32m     17\u001b[0m             FewShotPromptTemplate\u001b[38;5;241m.\u001b[39mcreate_example(\n\u001b[0;32m     18\u001b[0m                 base_prompt\u001b[38;5;241m.\u001b[39mtemplate,\n\u001b[0;32m     19\u001b[0m                 data_validation_prompt,\n\u001b[0;32m     20\u001b[0m             ),\n\u001b[0;32m     21\u001b[0m         ],\n\u001b[0;32m     22\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m.\u001b[39mtemplate,\n\u001b[0;32m     23\u001b[0m         suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSQL Query:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m         example_prompt\u001b[38;5;241m=\u001b[39mbase_prompt,\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'FewShotPromptTemplate' has no attribute 'create_example'"
     ]
    }
   ],
   "source": [
    "# Combine the prompts using FewShotPromptTemplate\n",
    "def generate_prompt(query):\n",
    "    prompt = base_prompt\n",
    "\n",
    "    # Analyze user query for keywords\n",
    "    keywords = [\"accuracy\", \"recall\", \"precision\", \"MAE\", 'model', 'best', 'worst']\n",
    "    detected_metrics = [metric for metric in keywords if metric in query.lower()]\n",
    "\n",
    "    # Add conditional prompts based on detected metrics\n",
    "    if detected_metrics:\n",
    "        prompt = FewShotPromptTemplate(\n",
    "            examples=[\n",
    "                FewShotPromptTemplate.create_example(\n",
    "                    base_prompt.template,\n",
    "                    metric_prompt,\n",
    "                ),\n",
    "                FewShotPromptTemplate.create_example(\n",
    "                    base_prompt.template,\n",
    "                    data_validation_prompt,\n",
    "                ),\n",
    "            ],\n",
    "            prefix=prompt.template,\n",
    "            suffix=\"\\nSQL Query:\",\n",
    "            example_prompt=base_prompt,\n",
    "        )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "prompt = generate_prompt(\"What is the accuracy of model A\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chain 1: Generate SQL Query\n",
    "generate_sql_chain = create_sql_query_chain(prompt=generate_prompt(\"dummy_query\"), llm=llm, db=db)\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "# Initialize Chain 2: Execute SQL Query and generate structured answer\n",
    "execute_sql_chain = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "def calculate_token_size(text):\n",
    "    # Split text into tokens and count the total number of tokens\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "def execute_combined_chain(question):\n",
    "    # Step 1: Generate SQL Query\n",
    "    sql_query = generate_sql_chain.invoke({\"question\": question, \"top_k\": 1})\n",
    "\n",
    "    # Step 2: Execute the SQL Query to get the result\n",
    "    sql_result = execute_query(sql_query)  # Ensure this returns the result of executing the SQL query\n",
    "\n",
    "    # Step 3: Pass the necessary inputs to the final chain and format the output to include both SQL query and result\n",
    "    final_response = execute_sql_chain.invoke({\"question\": question, \"query\": sql_query, \"result\": sql_result})\n",
    "\n",
    "    # Calculate token size of input and output\n",
    "    input_token_size = calculate_token_size(question)\n",
    "    output_token_size = calculate_token_size(final_response)\n",
    "    total_token_size = input_token_size + output_token_size\n",
    "    print(total_token_size)\n",
    "    \n",
    "    # Prompt user if total token size exceeds 10K tokens\n",
    "    if total_token_size > 10000:\n",
    "        return \"Your query is too complex. Please try asking in a simpler way or split it into multiple questions.\"\n",
    "\n",
    "    # Format the final answer to include both the SQL query and its result\n",
    "    final_answer = f\"SQL Query: {sql_query}\\n Answer: {final_response}\"\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "SQL Query: SELECT Model_Name, Model_Version, MAX(Performance_Metrics) as Best_Performance\n",
      "FROM models\n",
      "GROUP BY Model_Name;\n",
      " Answer: The best model versions for each model are as follows: Model 1 version 1, Model 10 version 2, Model 11 version 1, Model 12 version 2, Model 13 version 3, Model 14 version 1, Model 15 version 1, Model 16 version 1, Model 17 version 2, Model 18 version 2, Model 19 version 1, Model 2 version 1, Model 20 version 2, Model 21 version 2, Model 22 version 1, Model 23 version 3, Model 24 version 3, Model 25 version 1, Model 26 version 2, Model 27 version 2, Model 28 version 3, Model 29 version 2, Model 3 version 3, Model 30 version 1, Model 31 version 3, Model 32 version 3, Model 33 version 2, Model 34 version 3, Model 35 version 3, Model 36 version 3, Model 37 version 1, Model 38 version 2, Model 39 version 1, Model 4 version 3, Model 40 version 1, Model 41 version 2, Model 42 version 3, Model 43 version 1, Model 44 version 1, Model 45 version 2, Model 46 version 1, Model 47 version 1, Model 48 version 3, Model 49 version 1, Model 5 version 2, Model 50 version 1, Model 6 version 1, Model 7 version 2, Model 8 version 3, and Model 9 version 1.\n"
     ]
    }
   ],
   "source": [
    "# Example invocation\n",
    "question = \"Which model version is better in each model\"\n",
    "final_answer = execute_combined_chain(question)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_prompt() missing 1 required positional argument: 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt_template\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mquery, table_info\u001b[38;5;241m=\u001b[39mtable_info)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Initialize Chain 1: Generate SQL Query\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m generate_sql_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Initialize Chain 2: Execute SQL Query and generate structured answer\u001b[39;00m\n\u001b[0;32m     99\u001b[0m execute_sql_chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39manswer_prompt, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_prompt() missing 1 required positional argument: 'query'"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.agents.tools import BaseTool\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "# SQLite database connection\n",
    "db = SQLDatabase.from_uri(\"sqlite:///dataset.db\")\n",
    "# GPT connection\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "# Get the table information\n",
    "table_info = db.table_info\n",
    "\n",
    "# Define the base prompt\n",
    "base_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\"],\n",
    "    template=\"\"\"You are a SQLite expert and Machine Learning Engineer. Given an input question, create a syntactically correct SQLite query to run, to answer the input question.\n",
    "    Use CTEs (Common Table Expressions) and window functions whenever possible to make the queries more readable and efficient.\n",
    "    Only use the following tables: {table_info}.\n",
    "    Question: {input}.\n",
    "    Generate the best SQL query to answer the question.\"\"\",\n",
    ")\n",
    "\n",
    "# Define the metric prompt\n",
    "metric_prompt = \"\"\"**Metric Selection Instructions:**\n",
    "* For questions about detailed performance metrics (accuracy, MAE, recall, precision), you MUST JOIN the 'models' table with the 'model_metrics_view' on the MODEL_ID column.\n",
    "* Use the 'model_metrics_view' to access the individual performance metrics (recall, precision, accuracy, MAE) based on the model type.\n",
    "* For Classification models, use accuracy, precision, and recall from model_metrics_view.\n",
    "* For Regression models, use MAE from model_metrics_view.\n",
    "* Do not use aggregate functions like MAX() on the performance metrics columns, as they are stored as JSON arrays. Instead, directly access the required metric from the 'model_metrics_view'.\n",
    "\n",
    "**Example:**\n",
    "Question: Which classification models have a recall greater than 0.8?\n",
    "WITH classification_models AS (\n",
    "  SELECT m.Model_Name, m.Model_Type, mmv.recall\n",
    "  FROM models m\n",
    "  JOIN model_metrics_view mmv ON m.MODEL_ID = mmv.MODEL_ID\n",
    "  WHERE m.Model_Type IN ('Multi-Class', 'Classification')\n",
    ")\n",
    "SELECT Model_Name, recall \n",
    "FROM classification_models\n",
    "WHERE CAST(recall AS FLOAT) > 0.8;\n",
    "\"\"\"\n",
    "\n",
    "# Define the data validation prompt\n",
    "data_validation_prompt = \"\"\"**Data Validation Instructions:**\n",
    "* Before executing the query, ensure that the referenced data likely exists in the database.\n",
    "* If potential issues are detected, indicate the problem and suggest alternatives instead of executing the query.\n",
    "Question: What is the accuracy of Model 10?\n",
    "\n",
    "     SELECT accuracy \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE lower(Model_Name) = 'model 10' and Model_type IN ('Multi-Class', 'Classification');\n",
    "    Answer: Model 10 is a Regression model type so it accuracy is not the right metric to calculate performace of the model\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to answer the questions\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question in a conversational tone.\n",
    "    If the SQL result is empty, provide a helpful message indicating that no matching data was found.\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Combine the prompts using FewShotPromptTemplate\n",
    "def generate_prompt(query):\n",
    "    prompt_template = FewShotPromptTemplate(\n",
    "        examples=[],\n",
    "        prefix=base_prompt.template,\n",
    "        suffix=\"SQL Query:\",\n",
    "        input_variables=[\"input\", \"table_info\"],\n",
    "        example_prompt=base_prompt,\n",
    "        example_separator=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    # Analyze user query for keywords\n",
    "    keywords = [\"accuracy\", \"recall\", \"precision\", \"MAE\", \"model\", \"best\", \"worst\"]\n",
    "    detected_metrics = [metric for metric in keywords if metric in query.lower()]\n",
    "\n",
    "    # Add conditional prompts based on detected metrics\n",
    "    if detected_metrics:\n",
    "        prompt_template = prompt_template.partial(\n",
    "            examples=[\n",
    "                {\"input\": metric_prompt, \"table_info\": table_info},\n",
    "                {\"input\": data_validation_prompt, \"table_info\": table_info},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return prompt_template.format(input=query, table_info=table_info)\n",
    "\n",
    "# Initialize Chain 1: Generate SQL Query\n",
    "generate_sql_chain = LLMChain(llm=llm, prompt=base_prompt, output_key=\"query\")\n",
    "\n",
    "# Initialize Chain 2: Execute SQL Query and generate structured answer\n",
    "execute_sql_chain = LLMChain(llm=llm, prompt=answer_prompt, output_key=\"result\")\n",
    "\n",
    "# Create a custom tool for executing SQL queries\n",
    "class QuerySQLDataBaseTool(BaseTool):\n",
    "    name = \"QuerySQLDataBase\"\n",
    "    description = \"Useful for querying a SQL database to obtain information.\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        return db.run(query)\n",
    "\n",
    "    async def _arun(self, query: str) -> str:\n",
    "        raise NotImplementedError(\"QuerySQLDataBaseTool does not support async\")\n",
    "\n",
    "# Create an agent with the custom tool\n",
    "tools = [QuerySQLDataBaseTool()]\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "def execute_combined_chain(question):\n",
    "    # Step 1: Generate SQL Query\n",
    "    prompt = generate_prompt(question)\n",
    "    sql_query = generate_sql_chain.run({\"input\": question, \"table_info\": table_info})\n",
    "\n",
    "    # Step 2: Execute the SQL Query to get the result\n",
    "    sql_result = agent.run(sql_query)\n",
    "\n",
    "    # Step 3: Pass the necessary inputs to the final chain and format the output\n",
    "    final_response = execute_sql_chain.run(\n",
    "        question=question, query=sql_query, result=sql_result\n",
    "    )\n",
    "\n",
    "    # Format the final answer to include both the SQL query and its result\n",
    "    final_answer = f\"SQL Query: {sql_query}\\nAnswer: {final_response}\"\n",
    "    return final_answer\n",
    "\n",
    "# Example invocation\n",
    "question = \"Which classification models have a recall greater than 0.8?\"\n",
    "final_answer = execute_combined_chain(question)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "SQL Query: WITH cte AS (\n",
      "  SELECT m.Company_Name, m.Model_Name, m.Model_Version, m.Model_Type,\n",
      "         CASE \n",
      "           WHEN m.Model_Type IN ('Multi-Class', 'Classification') THEN json_extract(m.Performance_Metrics, '$[0]')\n",
      "           WHEN m.Model_Type = 'Regression' THEN json_extract(m.Performance_Metrics, '$[0]')\n",
      "         END AS metric,\n",
      "         ROW_NUMBER() OVER (PARTITION BY m.Company_Name ORDER BY \n",
      "                              CASE \n",
      "                                WHEN m.Model_Type IN ('Multi-Class', 'Classification') THEN json_extract(m.Performance_Metrics, '$[0]')\n",
      "                                WHEN m.Model_Type = 'Regression' THEN -json_extract(m.Performance_Metrics, '$[0]')\n",
      "                              END DESC) AS rn\n",
      "  FROM models m\n",
      ")\n",
      "SELECT Company_Name, Model_Name, Model_Version, Model_Type, metric\n",
      "FROM cte\n",
      "WHERE rn = 1;\n",
      "Answer: The best performing models for each company are as follows: For Allstate, it's 'Model 20' version 2, which is a Classification model with a performance metric of 0.95. For Liberty Mutual, the best performing model is 'Model 3' version 3, a Multi-Class model with a performance metric of 0.95. Nationwide's best model is 'Model 1' version 3, a Multi-Class model with a performance metric of 0.95. Lastly, for State Farm, the best performing model is 'Model 40' version 2, a Classification model with a performance metric of 0.95.\n"
     ]
    }
   ],
   "source": [
    "# SQLite database connection\n",
    "db = SQLDatabase.from_uri(\"sqlite:///dataset.db\")\n",
    "\n",
    "# GPT connection\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Define the base prompt\n",
    "base_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"\"\"You are a SQLite expert and Machine Learning Engineer. Given an input question, create a syntactically correct SQLite query to run, to answer the input question. \n",
    "    * Do not use aggregate functions like MAX() on the performance metrics columns, as they are stored as JSON arrays. Instead, directly access the required metric from the 'model_metrics_view'.\n",
    "\\n\n",
    "**Example:**  \n",
    "\n",
    "Question: Give company-wise best performing models with metrics.\n",
    "WITH cte AS (\n",
    "  SELECT m.Company_Name, m.Model_Name, m.Model_Version, m.Model_Type,\n",
    "         CASE \n",
    "           WHEN m.Model_Type IN ('Multi-Class', 'Classification') THEN mv.accuracy\n",
    "           WHEN m.Model_Type = 'Regression' THEN mv.MAE\n",
    "         END AS metric,\n",
    "         ROW_NUMBER() OVER (PARTITION BY m.Company_Name ORDER BY \n",
    "                              CASE \n",
    "                                WHEN m.Model_Type IN ('Multi-Class', 'Classification') THEN mv.accuracy\n",
    "                                WHEN m.Model_Type = 'Regression' THEN -mv.MAE\n",
    "                              END DESC) AS rn\n",
    "  FROM models m\n",
    "  JOIN model_metrics_view mv ON m.MODEL_ID = mv.MODEL_ID\n",
    ")\n",
    "SELECT Company_Name, Model_Name, Model_Version, Model_Type, metric\n",
    "FROM cte\n",
    "WHERE rn = 1;\n",
    "\n",
    "\\n Only use the following tables: {table_info}.Question: {input}.Generate up to {top_k} SQL queries to answer the question.\"\"\",\n",
    ")\n",
    "\n",
    "# Define the metric prompt\n",
    "metric_prompt = \"\"\"**Metric Selection Instructions:**\n",
    "* For questions about detailed performance metrics (accuracy, MAE, recall, precision), you MUST JOIN the 'models' table with the 'model_metrics_view' using a CTE (Common Table Expression).\n",
    "* Use window functions (e.g., ROW_NUMBER(), RANK()) to handle multiple model versions and select the best metrics for each model.\n",
    "* Handle the performance metrics column correctly by extracting the appropriate metric based on the model type.\n",
    "* Below are the examples for you to learn on how to write effective code and on how to join the table with model_metrics_view\n",
    "\n",
    "**Example:**\n",
    "Question: Which models have the highest recall for each model name?\n",
    "WITH cte AS (\n",
    "  SELECT m.Model_Name, m.Model_Version, mv.recall,\n",
    "         ROW_NUMBER() OVER (PARTITION BY m.Model_Name ORDER BY mv.recall DESC) AS rn\n",
    "  FROM models m\n",
    "  JOIN model_metrics_view mv ON m.MODEL_ID = mv.MODEL_ID\n",
    "  WHERE m.Model_type IN ('Multi-Class', 'Classification')\n",
    ")\n",
    "\n",
    "SELECT Model_Name, Model_Version, recall\n",
    "FROM cte\n",
    "WHERE rn = 1;\n",
    "\n",
    "Question: What is the accuracy of Model 10?\n",
    "SELECT accuracy \n",
    "         FROM models \n",
    "         JOIN model_metrics_view ON models.MODEL_ID = model_metrics_view.MODEL_ID\n",
    "         WHERE lower(Model_Name) = 'model 10' and Model_type IN ('Multi-Class', 'Classification');\n",
    "Answer: Model 10 is a Regression model type so it accuracy is not the right metric to calculate performace of the model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define the data validation prompt\n",
    "data_validation_prompt = \"\"\"**Data Validation Instructions:**\n",
    "* Before executing the query, ensure that the referenced data likely exists in the database.\n",
    "* If potential issues are detected, indicate the problem and suggest alternatives instead of executing the query.\n",
    "\n",
    "Question: What is the volume of Model A in CCC company ?\n",
    "SQL Query:    SELECT SUM(Daily_Volume) AS total_volume \n",
    "         FROM models \n",
    "         WHERE lower(Model_Name) = 'model a' and lower(Company_Name) == 'ccc';\n",
    "    Answer: It seems there is no model named 'Model A' and no company called CCC in the database.  Can you please try with a different model and company name?\n",
    "\"\"\"\n",
    "# Combine the base prompt with the data validation prompt\n",
    "combined_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
    "    template=base_prompt.template + \"\\n\\n\" + data_validation_prompt,\n",
    ")\n",
    "\n",
    "# Prompt to answer the questions\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question in conversational tone. If the SQL result is empty, provide a helpful message indicating that no matching data was found.\n",
    "\n",
    "    {question}  <-- Notice the removal of \"Question:\" \n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Combine the prompts using FewShotPromptTemplate\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Predefined reference questions related to performance metrics and model comparison\n",
    "reference_questions = [\n",
    "    \"What is the accuracy of the model?\",\n",
    "    \"Which model has the highest precision?\",\n",
    "    \"Compare the recall of different models.\",\n",
    "    \"What is the MAE of the regression model?\",\n",
    "    \"Which model performs best in terms of accuracy?\",\n",
    "]\n",
    "\n",
    "# Combine the prompts using FewShotPromptTemplate\n",
    "def generate_prompt(query):\n",
    "    prompt = base_prompt \n",
    "    # Analyze user query using semantic search\n",
    "    if should_include_metric_prompt(query):\n",
    "        prompt = FewShotPromptTemplate(\n",
    "            examples=[\n",
    "                {\"query\": base_prompt.template, \"context\": metric_prompt},\n",
    "                {\"query\": base_prompt.template, \"context\": data_validation_prompt},\n",
    "            ],\n",
    "            example_prompt=base_prompt,\n",
    "            suffix=\"\\nSQL Query:\",\n",
    "            input_variables=[\"query\"],\n",
    "        )\n",
    "    return prompt\n",
    "\n",
    "def should_include_metric_prompt(query):\n",
    "    # Encode the user's question and reference questions\n",
    "    query_embedding = model.encode(query)\n",
    "    reference_embeddings = model.encode(reference_questions)\n",
    "\n",
    "    # Compute the cosine similarity between the user's question and reference questions\n",
    "    similarity_scores = util.cos_sim(query_embedding, reference_embeddings)\n",
    "\n",
    "    # Check if any similarity score exceeds a threshold (e.g., 0.7)\n",
    "    if (similarity_scores > 0.7).any():\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Initialize Chain 1: Generate SQL Query\n",
    "generate_sql_chain = create_sql_query_chain(prompt=generate_prompt(\"dummy_query\"), llm=llm, db=db)\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "# Initialize Chain 2: Execute SQL Query and generate structured answer\n",
    "execute_sql_chain = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "def calculate_token_size(text):\n",
    "    # Split text into tokens and count the total number of tokens\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "def execute_combined_chain(question):\n",
    "    # Step 1: Generate SQL Query\n",
    "    sql_query = generate_sql_chain.invoke({\"question\": question, \"top_k\": 1})\n",
    "    \n",
    "    # Step 2: Execute the SQL Query to get the result\n",
    "    sql_result = execute_query(sql_query)\n",
    "    \n",
    "    # Step 3: Pass the necessary inputs to the final chain and format the output to include both SQL query and result\n",
    "    final_response = execute_sql_chain.invoke({\"question\": question, \"query\": sql_query, \"result\": sql_result})\n",
    "    \n",
    "    # Calculate token size of input and output\n",
    "    input_token_size = calculate_token_size(question)\n",
    "    output_token_size = calculate_token_size(final_response)\n",
    "    total_token_size = input_token_size + output_token_size\n",
    "    print(total_token_size)\n",
    "    \n",
    "    # Prompt user if total token size exceeds 10K tokens\n",
    "    if total_token_size > 10000:\n",
    "        return \"Your query is too complex. Please try asking in a simpler way or split it into multiple questions.\"\n",
    "    \n",
    "    # Format the final answer to include both the SQL query and its result\n",
    "    final_answer = f\"SQL Query: {sql_query}\\nAnswer: {final_response}\"\n",
    "    return final_answer\n",
    "\n",
    "# Example invocation\n",
    "question = \"give company wise best performing model\"\n",
    "final_answer = execute_combined_chain(question)\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
